{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "DIGIT RECOGNIZER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishaanwho/Ishaanwho/blob/main/DIGIT_RECOGNIZER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSo2uB4KVps_"
      },
      "source": [
        "DIGIT RECOGNIZER (MNIST dataset) :\n",
        "I have implemented this dataset using CNN in Tensorflow. I have used 2 Convolution layers and 2 MaxPooling layers. Images which are initially flattened and are of 784 pixels are then converted to 2D image of 28*28 pixels.After passing through convolution and maxpooling layers we then flattten it again and pass through a dense layer to bring some uncertainity. Further we pass it through a dropout layer which makes some units inactive and has keep_probability of 0.8 It is done for Regularisation and helps to avoid overfitting of data. Further it goes to our output layer which gives output as labels of 0-9. Using argmax we get the argument no. of max value and hence it is our predicted number.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3uPni-8S3cp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MZQa0jCVptD"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKGtCGwZVptL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "cfe015ac-5f86-4a94-9a75-4fa5218d6924"
      },
      "source": [
        "# i have taken the mnist data input from tensorflow\n",
        "from tensorflow.examples.tutorials.mnist import input_name\n",
        "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
        "# one_hot means that we have encoded the digit as labels from 0-9 "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b194982d2e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# i have taken the mnist data input from tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# one_hot means that we have encoded the digit as labels from 0-9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozDn_oNZVptg"
      },
      "source": [
        "#we will convert our 784 pixel input to 2_D image of 28 * 28 pixels\n",
        "input_width=28\n",
        "input_height=28\n",
        "input_channels=1\n",
        "input_pixels=784\n",
        "# I have used 2 convolution and 2 maxpooling layers in my CNN model\n",
        "#first convolution layer has 32 units and second has 64 units\n",
        "# the filter for a unit is of size k*k and stride is 1 and padding=same\n",
        "# filter for maxpooling layer is 2*2\n",
        "n_conv1=32\n",
        "n_conv2=64\n",
        "stride_conv1=1\n",
        "stride_conv2=1\n",
        "k_conv1=5\n",
        "k_conv2=5\n",
        "k_maxpool1=2\n",
        "k_maxpool2=2\n",
        "# number of units in hidden layer is 1024\n",
        "#output layer has 10 units\n",
        "n_hidden=1024\n",
        "n_out=10\n",
        "\n",
        "input_size_to_hidden=(input_width)//(k_maxpool1*k_maxpool2)*(input_height)//(k_maxpool1*k_maxpool2)*n_conv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfRROPZXVptp"
      },
      "source": [
        "# weights and biases for each layer are generated using Tensorflow \n",
        "weights={\n",
        "    'wcl1':tf.Variable(tf.random_normal([k_conv1,k_conv1,input_channels,n_conv1])),\n",
        "    'wcl2':tf.Variable(tf.random_normal([k_conv2,k_conv2,n_conv1,n_conv2])),\n",
        "    'whl' :tf.Variable(tf.random_normal([input_size_to_hidden,n_hidden])),\n",
        "    'out' :tf.Variable(tf.random_normal([n_hidden,n_out]))\n",
        "}\n",
        "\n",
        "biases={\n",
        "    'bcl1':tf.Variable(tf.random_normal([n_conv1])),\n",
        "    'bcl2':tf.Variable(tf.random_normal([n_conv2])),\n",
        "    'bhl':tf.Variable(tf.random_normal([n_hidden])),\n",
        "    'out':tf.Variable(tf.random_normal([n_out])),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5-EhwegVptv"
      },
      "source": [
        "# here the conv function adds weights and biases to our input data and does all of stride and padding\n",
        "def conv(x, weights, bias, strides):\n",
        "    out = tf.nn.conv2d(x, weights, padding=\"SAME\", strides = [1, strides, strides, 1])\n",
        "    out = tf.nn.bias_add(out, bias)\n",
        "    out = tf.nn.relu(out)\n",
        "    return out\n",
        "# maxpooling function does the pooling which is used to avoid mainly overfitting in our model \n",
        "def maxpooling(x, k):\n",
        "    return tf.nn.max_pool(x, padding = \"SAME\", ksize = [1, k, k, 1], strides = [1, k, k, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68gSwdAMVpt8"
      },
      "source": [
        "#cnn function defines all of our convolution and maxpooling layers\n",
        "def cnn(x,weights,biases,keep_prob):\n",
        "    x=tf.reshape(x,shape=[-1,input_height,input_width,input_channels])\n",
        "    conv1=conv(x,weights['wcl1'],biases['bcl1'],stride_conv1)\n",
        "    conv1_pool=maxpooling(conv1,k_maxpool1)\n",
        "    \n",
        "    conv2=conv(conv1_pool,weights['wcl2'],biases['bcl2'],stride_conv2)\n",
        "    conv2_pool=maxpooling(conv2,k_maxpool2)\n",
        "    \n",
        "# now here we have our hidden layers that applies relu activation function\n",
        "# we also have a dropout layer that brings some uncertainity needed and has keep probability of 0.8 \n",
        "    hidden_input=tf.reshape(conv2_pool,shape=[-1,input_size_to_hidden])\n",
        "    hidden_output_before_activation=tf.add(tf.matmul(hidden_input,weights['whl']),biases['bhl'])\n",
        "    hidden_output_before_dropout=tf.nn.relu(hidden_output_before_activation)\n",
        "    hidden_output=tf.nn.dropout(hidden_output_before_dropout,keep_prob)\n",
        "    \n",
        "    output=tf.add(tf.matmul(hidden_output,weights['out']),biases['out'])\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luLuR65TVpuB"
      },
      "source": [
        "x = tf.placeholder(\"float\", [None, input_pixels])\n",
        "y = tf.placeholder(tf.int32, [None, n_out])\n",
        "keep_prob = tf.placeholder(\"float\")\n",
        "pred = cnn(x, weights, biases, keep_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygqnqbyCVpuG"
      },
      "source": [
        "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels=y))\n",
        "#this function measures the softmax cross entropy between the predictions and actual label y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhX8CkzWVpuR"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "optimize = optimizer.minimize(cost)\n",
        "#this optimizer runs the Adam algorithm and minimizes the cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRItk9wmVpuf"
      },
      "source": [
        "sess=tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOeV8AwGVpuq"
      },
      "source": [
        "# By providing all data at one go we can lead to overfitting of data \n",
        "#so to avoid it we pass input images in batches rather than all at once\n",
        "# for every batch it calculates cost and we see that cost keeps decreasing(optimizing)\n",
        "#mnist.train.next_batch itself provides the batch of given size \n",
        "batch_size = 100\n",
        "for i in range(25):\n",
        "    num_batches = int(mnist.train.num_examples/batch_size)\n",
        "    total_cost = 0\n",
        "    for j in range(num_batches):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        c, _ = sess.run([cost,optimize], feed_dict={x:batch_x , y:batch_y, keep_prob:0.8})\n",
        "        total_cost += c\n",
        "    print(total_cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJxX0ITNVpu1"
      },
      "source": [
        "# we then get predictions in binary format for 10 classes\n",
        "# by getting argument of max value we get our prediction\n",
        "predictions=tf.argmax(pred,1)\n",
        "correct_labels=tf.argmax(y,1)\n",
        "correct_predictions=tf.equal(predictions,correct_labels)\n",
        "predictions,correct_pred=sess.run([predictions,correct_predictions],feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0})\n",
        "correct_pred.sum()\n",
        "#tf.equal gives all those predictions which are equal to our correct_labels\n",
        "#out of 10000 images we have predicted 9841 correct (98.41 % accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1_TExsmjaFj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzylI46CjbRK"
      },
      "source": [
        ""
      ]
    }
  ]
}